Instruction
•	Necessary file will be in the folder with experiment number


1)Implementation of Noise removal algorithms using OpenCV
import cv2
from matplotlib import pyplot as plt
img=cv2.imread(‘C:/Users/thara/Downloads/bear.jpeg’)
import numpy as np
dst = cv2.fastNlMeansDenoisingColored(img, None, 10, 10, 7, 15)
plt.subplot(121), plt.imshow(img)
plt.subplot(122), plt.imshow(dst)
plt.show()

2)Implementation of Object detection based on Edge detection algorithms on any application using OpenCV
import cv2
import numpy as np
# Read the image
original_img = cv2.imread('C:/Users/thara/Downloads/bear.jpeg', cv2.IMREAD_COLOR)
# Check if the image was successfully loaded
if original_img is None:
    print("Error: Unable to load image.")
else:
    # Convert to grayscale
    gray = cv2.cvtColor(original_img, cv2.COLOR_BGR2GRAY)
    # Apply Gaussian Blur
    blur_img = cv2.GaussianBlur(gray, (3, 3), 0)
    # Detect edges using Canny
    edges = cv2.Canny(image=blur_img, threshold1=100, threshold2=200)
    # Display the results
    cv2.imshow('Original Image', original_img)
    cv2.imshow('Edges', edges)
    cv2.waitKey(0)
    cv2.destroyAllWindows()

3) Implementation of Perspective projection of the lane borders using OpenCV

from google.colab.patches import cv2_imshow
import cv2
import numpy as np
def detect_lane_border(image):
    # Convert the image to grayscale
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    # Apply Gaussian blur to remove noise
    blurred = cv2.GaussianBlur(gray, (7, 7), 0)

    # Apply Canny edge detection
    edges = cv2.Canny(blurred, 50, 100)

    # Define region of interest (ROI)
    height, width = edges.shape
    mask = np.zeros_like(edges)
    region_of_interest_vertices = [
        (0, height),
        (width / 2, height / 2),
        (width, height)
    ]
    cv2.fillPoly(mask, [np.array(region_of_interest_vertices, np.int32)], 255)
    masked_edges = cv2.bitwise_and(edges, mask)

    # Detect lines using Hough transform
    lines = cv2.HoughLinesP(masked_edges, rho=1, theta=np.pi/180, threshold=50, minLineLength=50, maxLineGap=100)

    # Draw the detected lines on the original image
    if lines is not None:
        for line in lines:
            x1, y1, x2, y2 = line[0]
            cv2.line(image, (x1, y1), (x2, y2), (255, 0, 0), 5)
    return image
# Read the input image
input_image = cv2.imread('/content/lane.jpeg')
# Detect lane borders
output_image = detect_lane_border(input_image)
# Display the result
cv2_imshow(output_image)

4) Implementation of Principal Component Analysis
import pandas as pd
import numpy as np
data=pd.read_csv("/content/drive/MyDrive/cv lab experiment/wine.csv")
data.head()
data.isnull().sum()
X = data.iloc[:, :-1].values
y = data.iloc[:, -1].values
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
from sklearn.svm import SVC
cls=SVC()
cls.fit(X_train,y_train)
from sklearn.metrics import confusion_matrix, accuracy_score
y_pred = cls.predict(X_test)
cm = confusion_matrix(y_test, y_pred)
print(cm)
accuracy_score(y_test, y_pred)
from sklearn.decomposition import PCA
pca = PCA(n_components = 2)
X_train = pca.fit_transform(X_train)
X_test = pca.transform(X_test)
from sklearn.svm import SVC
cls=SVC()
cls.fit(X_train,y_train)
from sklearn.metrics import confusion_matrix, accuracy_score
y_pred = cls.predict(X_test)
cm = confusion_matrix(y_test, y_pred)
print(cm)
accuracy_score(y_test, y_pred)

5) Implementation of Feature Extraction of an object using SIFT in OpenCV
import cv2
import matplotlib.pyplot as plt
image = cv2.imread("/content/bear.jpeg")
img = cv2.imread('/content/bear.jpeg')
gray= cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
sift = cv2.SIFT_create()
kp = sift.detect(gray, None)
img=cv2.drawKeypoints(gray ,kp ,img ,flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)
plt.figure(figsize=(25, 7))
plt.subplot(121).imshow(image)
plt.subplot(122).imshow(img)
plt.show()

6) Implementation of Feature Extraction of an object using SURF in OpenCV
from google.colab.patches import cv2_imshow
import cv2
import numpy as np
# Load images
img_scene = cv2.imread("/content/surf_.png", cv2.IMREAD_GRAYSCALE)  # Image containing the scene
img_object = cv2.imread("/content/surf_img.jpeg", cv2.IMREAD_GRAYSCALE)  # Image of the object to be detected
# Check if the images are loaded successfully
if img_scene is None:
    print("Error: Unable to load scene image.")
elif img_object is None:
    print("Error: Unable to load object image.")
else:
    # Create ORB detector
    orb = cv2.ORB_create()
    # Detect keypoints and descriptors for the object and scene images
    keypoints_object, descriptors_object = orb.detectAndCompute(img_object, None)
    keypoints_scene, descriptors_scene = orb.detectAndCompute(img_scene, None)
    # Create a Brute-Force Matcher object
    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)
    # Match descriptors of the object and scene images
    matches = bf.match(descriptors_object, descriptors_scene)
    # Sort the matches based on distance
    matches = sorted(matches, key=lambda x: x.distance)
    # Draw the first 10 matches
    img_matches = cv2.drawMatches(img_object, keypoints_object, img_scene, keypoints_scene, matches[:10], None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)
    # Display the matched keypoints
    cv2_imshow(img_matches)

7) Implementation of Emotion Recognition in OpenCV
import cv2
import numpy as np
from tensorflow.keras.models import load_model
# Load the trained model
model = load_model("C:/Users/thara/Downloads/model.h5")
# Prevent OpenCL usage and unnecessary logging messages
cv2.ocl.setUseOpenCL(False)

# Dictionary mapping class labels with corresponding emotions
emotion_dict = {0: "Angry", 1: "Disgusted", 2: "Fearful", 3: "Happy", 4: "Neutral", 5: "Sad", 6: "Surprised"}

# Start the webcam feed
cap = cv2.VideoCapture(0)  # You can also specify a video file path instead of 0 for webcam

while True:
    # Capture frame
    ret, frame = cap.read()
    if not ret:
        break

    # Find Haar cascade to draw bounding box around face
    face_casc = cv2.CascadeClassifier('C:/Users/thara/Downloads/Emotion-detection-master/Emotion-detection-master/src/haarcascade_frontalface_default.xml')
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    faces = face_casc.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)

    for (x, y, w, h) in faces:
        cv2.rectangle(frame, (x, y-50), (x+w, y+h+10), (255, 0, 0), 2)
        roi_gray = gray[y:y + h, x:x + w]
        
        # Resize and convert to color image
        roi_color = cv2.resize(roi_gray, (112, 112))
        roi_color = cv2.cvtColor(roi_color, cv2.COLOR_GRAY2RGB)
        
        # Normalize
        roi_color = roi_color / 255.0
        
        # Expand dimensions and make prediction
        cropped_img = np.expand_dims(roi_color, axis=0)
        prediction = model.predict(cropped_img)
        
        maxindex = int(np.argmax(prediction))
        cv2.putText(frame, emotion_dict[maxindex], (x, y), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)

    # Display the frame
    cv2.imshow('Video', cv2.resize(frame, (500, 500), interpolation=cv2.INTER_CUBIC))
    
    # Break the loop if 'q' is pressed
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# Release the webcam and close OpenCV windows
cap.release()
cv2.destroyAllWindows()

8) Implementation of Gesture Recognition in OpenCV


import cv2
import numpy as np

# Load the pre-trained Haar Cascade hand detection model
hand_cascade = cv2.CascadeClassifier('C:/Users/thara/Downloads/haarcascade_hand.xml')

# Function to detect and recognize hand gestures
def detect_gestures(frame):
    # Convert the frame to grayscale
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

    # Detect hands in the frame
    hands = hand_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))

    # Draw lines and dots representing the detected hands
    for (x, y, w, h) in hands:
        # Calculate the center of the hand
        center_x = x + w // 2
        center_y = y + h // 2

        # Draw a crosshair at the center of the hand
        cv2.line(frame, (center_x - 10, center_y), (center_x + 10, center_y), (0, 255, 0), 2)
        cv2.line(frame, (center_x, center_y - 10), (center_x, center_y + 10), (0, 255, 0), 2)

        # Draw a circle around the hand
        cv2.circle(frame, (center_x, center_y), max(w, h) // 2, (0, 255, 0), 2)

    return frame

# Main function for capturing video from webcam
def main():
    cap = cv2.VideoCapture(0)

    if not cap.isOpened():
        print("Error: Unable to access the webcam.")
        return

    while True:
        ret, frame = cap.read()
        if not ret:
            print("Error: Failed to capture frame.")
            break

        # Detect and recognize gestures in the frame
        frame = detect_gestures(frame)

        cv2.imshow('Gesture Recognition', frame)

        if cv2.waitKey(1) & 0xFF == ord('q'):  # Press 'q' to exit
            break

    cap.release()
    cv2.destroyAllWindows()

if __name__ == "__main__":
    main()


9) Implementation of Face Detection in OpenCV

import cv2

# Load the pre-trained Haar Cascade face detection model
face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')

# Function to detect faces in a frame
def detect_faces(frame):
    # Convert the frame to grayscale
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

    # Detect faces in the frame
    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))

    # Draw rectangles around the detected faces
    for (x, y, w, h) in faces:
        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)

    return frame

# Main function for capturing video from the camera
def main():
    cap = cv2.VideoCapture(0)  # Use default camera (index 0)

    if not cap.isOpened():
        print("Error: Unable to access the camera.")
        return

    while True:
        ret, frame = cap.read()
        if not ret:
            print("Error: Failed to capture frame.")
            break

        # Detect faces in the frame
        frame = detect_faces(frame)

        # Display the resulting frame
        cv2.imshow('Face Detection', frame)

        # Press 'q' to exit the video feed
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    # Release the camera and close all OpenCV windows
    cap.release()
    cv2.destroyAllWindows()

if __name__ == "__main__":
    main()

10) Implementation of Object detection using AdaBoost in OpenCV

pip install opencv-python==4.5.5.64


import cv2
# Load pre-trained AdaBoost object detection model
object_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
# Function to detect objects using AdaBoost
def detect_objects(frame):
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    objects = object_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))

    for (x, y, w, h) in objects:
        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)

    return frame
# Main function for capturing video from webcam
def main():
    cap = cv2.VideoCapture(0)

    while True:
        ret, frame = cap.read()
        if not ret:
            break
        frame = detect_objects(frame)
        cv2.imshow('Object Detection using AdaBoost', frame)
        if cv2.waitKey(1) & 0xFF == ord('a'):
            break
    cap.release()
    cv2.destroyAllWindows()

if __name__ == "__main__":
    main()

                                                           or
import cv2
import numpy as np

# Load YOLOv3 model and configuration files
net = cv2.dnn.readNet('yolov3.weights', 'yolov3.cfg')
with open('coco.names', 'r') as f:
    classes = f.read().strip().split('\n')

# Function to perform object detection on a frame
def detect_objects(frame):
    # Create a blob from the input image
    blob = cv2.dnn.blobFromImage(frame, 1/255.0, (416, 416), swapRB=True, crop=False)

    # Set the input for the network and perform a forward pass
    net.setInput(blob)
    layer_outputs = net.forward(net.getUnconnectedOutLayersNames())

    # Process the detections
    conf_threshold = 0.5
    nms_threshold = 0.4
    boxes = []
    confidences = []
    class_ids = []

    for output in layer_outputs:
        for detection in output:
            scores = detection[5:]
            class_id = np.argmax(scores)
            confidence = scores[class_id]
            if confidence > conf_threshold:
                center_x = int(detection[0] * frame.shape[1])
                center_y = int(detection[1] * frame.shape[0])
                width = int(detection[2] * frame.shape[1])
                height = int(detection[3] * frame.shape[0])
                left = int(center_x - width / 2)
                top = int(center_y - height / 2)
                boxes.append([left, top, width, height])
                confidences.append(float(confidence))
                class_ids.append(class_id)

    # Apply non-maximum suppression to remove overlapping boxes
    indices = cv2.dnn.NMSBoxes(boxes, confidences, conf_threshold, nms_threshold)

    # Draw bounding boxes and labels on the frame
    if len(indices) > 0:
        for i in indices.flatten():
            (x, y, w, h) = boxes[i]
            label = f"{classes[class_ids[i]]}: {confidences[i]:.2f}"
            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)
            cv2.putText(frame, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

    return frame

# Main function for capturing video from the camera
def main():
    cap = cv2.VideoCapture(0)  # Use default camera (index 0)

    if not cap.isOpened():
        print("Error: Unable to access the camera.")
        return
   
    while True:
        ret, frame = cap.read()
        if not ret:
            print("Error: Failed to capture frame.")
            break

        # Perform object detection on the frame
        frame = detect_objects(frame)

        # Display the resulting frame
        cv2.imshow('Object Detection', frame)

        # Press 'q' to exit the video feed
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    # Release the camera and close all OpenCV windows
    cap.release()
    cv2.destroyAllWindows()

if __name__ == "__main__":
    main()
